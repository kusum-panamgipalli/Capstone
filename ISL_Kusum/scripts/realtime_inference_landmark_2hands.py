"""
Real-time ISL interpreter using LANDMARK-BASED model with 2-HAND SUPPORT!
Detects and processes up to 2 hands simultaneously for complex ISL signs
"""
import cv2
import numpy as np
import json
import time
from collections import deque
from tensorflow import keras
import mediapipe as mp

class ISLInterpreter2Hands:
    def __init__(self, model_path='../models/isl_landmark_model_2hands.h5', labels_path='../models/isl_landmark_labels_2hands.json'):
        print("Initializing MediaPipe hand detection (2-hand support)...")
        
        # MediaPipe hands - NOW DETECTS UP TO 2 HANDS!
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,  # Detect up to 2 hands!
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        print("‚úì MediaPipe initialized (max 2 hands)")
        
        print("Loading 2-hand landmark model...")
        self.model = keras.models.load_model(model_path, compile=False)
        print("‚úì 2-hand model loaded")
        
        print("Loading model info...")
        with open(labels_path, 'r') as f:
            model_info = json.load(f)
        
        self.class_names = model_info['class_names']
        self.X_mean = np.array(model_info['mean'], dtype=np.float32)
        self.X_std = np.array(model_info['std'], dtype=np.float32)
        
        print(f"‚úì Loaded {len(self.class_names)} classes")
        
        self.prediction_history = deque(maxlen=3)
        self.stable_prediction = None
        self.stable_count = 0
        self.confidence_threshold = 0.80
        self.stability_threshold = 5
        
        self.translation = ""
        self.inference_times = deque(maxlen=30)
        self.landmark_extraction_times = deque(maxlen=30)
        self.hands_detected = 0
        
    def extract_landmarks(self, frame):
        """
        Extract hand landmarks using MediaPipe (UP TO 2 HANDS!)
        Returns 126-element array (2 hands √ó 21 landmarks √ó 3 coordinates)
        If only 1 hand: pads second hand with zeros
        """
        start = time.time()
        
        # Convert to RGB for MediaPipe
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(rgb_frame)
        
        extraction_time = (time.time() - start) * 1000
        self.landmark_extraction_times.append(extraction_time)
        
        if results.multi_hand_landmarks:
            num_hands_detected = len(results.multi_hand_landmarks)
            
            # Only process up to 2 hands (model limitation)
            self.hands_detected = min(num_hands_detected, 2)
            
            # Extract all landmarks (up to 2 hands)
            all_landmarks = []
            
            for i, hand_landmarks in enumerate(results.multi_hand_landmarks):
                if i >= 2:  # Skip hands beyond the first 2
                    break
                    
                # Extract coordinates for this hand
                hand_coords = []
                for landmark in hand_landmarks.landmark:
                    hand_coords.extend([landmark.x, landmark.y, landmark.z])
                all_landmarks.extend(hand_coords)
            
            # Pad with zeros if only 1 hand detected
            if self.hands_detected == 1:
                all_landmarks.extend([0.0] * 63)  # Pad second hand
            
            # Only return the first 2 hands for visualization
            hand_landmarks_to_draw = results.multi_hand_landmarks[:2]
            
            return np.array(all_landmarks, dtype=np.float32), hand_landmarks_to_draw
        
        self.hands_detected = 0
        return None, None
    
    def predict_sign(self, landmarks):
        """Predict sign from landmarks (1 or 2 hands)"""
        start = time.time()
        
        # Normalize landmarks
        landmarks_normalized = (landmarks - self.X_mean) / (self.X_std + 1e-8)
        
        # Reshape for model input
        landmarks_batch = np.expand_dims(landmarks_normalized, axis=0)
        
        # Predict
        predictions = self.model.predict(landmarks_batch, verbose=0)[0]
        
        inference_time = (time.time() - start) * 1000
        self.inference_times.append(inference_time)
        
        predicted_idx = np.argmax(predictions)
        confidence = predictions[predicted_idx]
        
        # Get top 3
        top3_idx = np.argsort(predictions)[-3:][::-1]
        top3_predictions = [(self.class_names[idx], predictions[idx]) for idx in top3_idx]
        
        return predicted_idx, confidence, top3_predictions
    
    def get_smooth_prediction(self, frame):
        """Get prediction with smoothing"""
        # Extract landmarks (1 or 2 hands)
        landmarks, hand_landmarks_list = self.extract_landmarks(frame)
        
        if landmarks is None:
            return None, 0.0, [], None
        
        # Predict
        predicted_idx, confidence, top3_predictions = self.predict_sign(landmarks)
        
        if confidence > self.confidence_threshold:
            self.prediction_history.append(predicted_idx)
            
            if len(self.prediction_history) == self.prediction_history.maxlen:
                unique, counts = np.unique(list(self.prediction_history), return_counts=True)
                most_common_idx = unique[np.argmax(counts)]
                most_common_class = self.class_names[most_common_idx]
                
                return most_common_class, confidence, top3_predictions, hand_landmarks_list
        
        return None, confidence, top3_predictions, hand_landmarks_list
    
    def update_translation(self, predicted_class):
        """Update translation with stability checking"""
        if predicted_class is None:
            return
        
        if predicted_class == self.stable_prediction:
            self.stable_count += 1
            
            if self.stable_count == self.stability_threshold:
                self.translation += predicted_class
                print(f"\n‚úì Added '{predicted_class}' to translation")
                print(f"Current translation: {self.translation}")
        else:
            self.stable_prediction = predicted_class
            self.stable_count = 1
    
    def draw_interface(self, frame, predicted_class, confidence, top3_predictions, hand_landmarks_list):
        """Draw the interface overlay"""
        height, width = frame.shape[:2]
        
        # Draw hand landmarks for ALL detected hands
        if hand_landmarks_list is not None:
            for idx, hand_landmarks in enumerate(hand_landmarks_list):
                # Different colors for left/right hand
                connection_color = (0, 255, 0) if idx == 0 else (0, 255, 255)
                landmark_color = (0, 255, 0) if idx == 0 else (0, 255, 255)
                
                self.mp_drawing.draw_landmarks(
                    frame,
                    hand_landmarks,
                    self.mp_hands.HAND_CONNECTIONS,
                    self.mp_drawing_styles.get_default_hand_landmarks_style(),
                    self.mp_drawing_styles.get_default_hand_connections_style()
                )
        
        # Performance metrics (top-left)
        avg_extraction = np.mean(self.landmark_extraction_times) if len(self.landmark_extraction_times) > 0 else 0
        avg_inference = np.mean(self.inference_times) if len(self.inference_times) > 0 else 0
        total_time = avg_extraction + avg_inference
        fps = 1000 / total_time if total_time > 0 else 0
        
        cv2.rectangle(frame, (10, 10), (380, 160), (0, 0, 0), -1)
        cv2.putText(frame, f"LANDMARK MODEL - 2-HAND SUPPORT!", (20, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        cv2.putText(frame, f"Landmark extraction: {avg_extraction:.1f}ms", (20, 55),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(frame, f"Model inference: {avg_inference:.2f}ms", (20, 75),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        cv2.putText(frame, f"Total: {total_time:.1f}ms", (20, 95),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        cv2.putText(frame, f"FPS: {fps:.1f}", (20, 115),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # Hand detection status
        if self.hands_detected == 0:
            hand_status = "‚úó No Hands"
            color = (0, 0, 255)
        elif self.hands_detected == 1:
            hand_status = "ü§ö 1 Hand Detected"
            color = (0, 255, 0)
        else:
            hand_status = "ü§öü§ö 2 HANDS DETECTED!"
            color = (0, 255, 255)
        
        cv2.putText(frame, hand_status, (20, 140),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # Prediction panel (right side)
        panel_x = width - 350
        panel_y = 50
        
        # Draw semi-transparent panel
        overlay = frame.copy()
        cv2.rectangle(overlay, (panel_x - 10, panel_y - 10), 
                     (width - 10, panel_y + 250), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # Main prediction
        if predicted_class:
            cv2.putText(frame, f"Sign: {predicted_class}", (panel_x, panel_y),
                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)
            cv2.putText(frame, f"{confidence*100:.0f}%", (panel_x, panel_y + 40),
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
            
            # Stability bar
            cv2.putText(frame, "Stability:", (panel_x, panel_y + 80),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            bar_width = 250
            bar_height = 20
            stability_pct = (self.stable_count / self.stability_threshold)
            filled_width = int(bar_width * stability_pct)
            
            cv2.rectangle(frame, (panel_x, panel_y + 90), 
                         (panel_x + bar_width, panel_y + 90 + bar_height), (100, 100, 100), -1)
            cv2.rectangle(frame, (panel_x, panel_y + 90), 
                         (panel_x + filled_width, panel_y + 90 + bar_height), (0, 255, 0), -1)
            cv2.rectangle(frame, (panel_x, panel_y + 90), 
                         (panel_x + bar_width, panel_y + 90 + bar_height), (255, 255, 255), 2)
        
        # Top 3 predictions
        cv2.putText(frame, "Top Predictions:", (panel_x, panel_y + 140),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        for i, (pred_class, pred_conf) in enumerate(top3_predictions):
            y_pos = panel_y + 170 + (i * 25)
            text = f"{i+1}. {pred_class}: {pred_conf*100:.0f}%"
            cv2.putText(frame, text, (panel_x, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        # Translation area at bottom
        translation_height = 120
        translation_area = np.zeros((translation_height, width, 3), dtype=np.uint8)
        
        cv2.rectangle(translation_area, (0, 0), (width, translation_height), (40, 40, 40), -1)
        
        cv2.putText(translation_area, "Translation:", (20, 35),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        max_chars_per_line = 60
        if len(self.translation) > max_chars_per_line:
            line1 = self.translation[-max_chars_per_line*2:-max_chars_per_line]
            line2 = self.translation[-max_chars_per_line:]
            cv2.putText(translation_area, line1, (20, 65),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(translation_area, line2, (20, 95),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
        else:
            cv2.putText(translation_area, self.translation, (20, 75),
                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)
        
        combined = np.vstack([frame, translation_area])
        
        cv2.putText(combined, "SPACE: space | BACKSPACE: delete | C: clear | Q: quit", 
                   (20, height + 110),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        return combined
    
    def run(self):
        print("\n" + "="*70)
        print("ISL INTERPRETER - 2-HAND LANDMARK SUPPORT!")
        print("="*70)
        print("‚ú® Now detects and processes up to 2 hands simultaneously")
        print("‚ú® Perfect for complex 2-handed ISL signs")
        print("‚ú® Shows both hand skeletons with different colors")
        print("\nAdvantages:")
        print("  - ULTRA FAST: ~10ms latency, 60-100 FPS")
        print("  - Supports 1-hand AND 2-hand signs automatically")
        print("  - Lighting independent (coordinates, not pixels)")
        print("  - Background independent (hand geometry only)")
        print("  - Shows hand skeleton in real-time")
        print("\nControls:")
        print("  SPACE      = Add space")
        print("  BACKSPACE  = Delete last character")
        print("  C          = Clear all translation")
        print("  Q          = Quit")
        print("="*70 + "\n")
        
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("‚ùå Cannot open webcam")
            return
        
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        print("‚úì Webcam opened successfully")
        print("‚úì Starting interpreter...\n")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                print("‚ùå Failed to grab frame")
                break
            
            frame = cv2.flip(frame, 1)
            
            # Get prediction from landmarks (1 or 2 hands)
            predicted_class, confidence, top3_predictions, hand_landmarks_list = self.get_smooth_prediction(frame)
            
            # Update translation
            self.update_translation(predicted_class)
            
            # Draw interface
            display_frame = self.draw_interface(frame, predicted_class, confidence, 
                                                top3_predictions, hand_landmarks_list)
            
            cv2.imshow('ISL Interpreter - 2-Hand Support (ULTRA FAST!)', display_frame)
            
            # Handle keyboard input
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                break
            elif key == ord(' '):
                self.translation += ' '
                print("Added space")
            elif key == 8:  # Backspace
                if self.translation:
                    self.translation = self.translation[:-1]
                    print("Deleted last character")
            elif key == ord('c'):
                self.translation = ""
                print("Cleared translation")
        
        cap.release()
        cv2.destroyAllWindows()
        
        avg_extraction = np.mean(self.landmark_extraction_times)
        avg_inference = np.mean(self.inference_times)
        total_time = avg_extraction + avg_inference
        
        print("\n" + "="*70)
        print("‚úì Interpreter stopped")
        print(f"Final translation: {self.translation}")
        print(f"\nPerformance Stats:")
        print(f"  Landmark extraction: {avg_extraction:.2f}ms")
        print(f"  Model inference: {avg_inference:.2f}ms")
        print(f"  Total time: {total_time:.2f}ms")
        print(f"  Average FPS: {1000/total_time:.1f}")
        print("="*70)

if __name__ == "__main__":
    interpreter = ISLInterpreter2Hands()
    interpreter.run()
